import os
import time

# Set the backend before importing Keras
os.environ["KERAS_BACKEND"] = "jax"
# Avoid memory fragmentation on JAX backend.
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.00"

import keras_nlp
import keras

# Configuration
# --- Model ---
model_id = "gemma2_instruct_2b_en"  # Base Gemma model
lora_name = "korean"  # Or your custom LoRA name
lora_rank = 2       #  LoRA rank used during fine-tuning
token_limit = 256    # Max sequence length

# --- Paths ---
#  1. If you downloaded the LoRA weights from Kaggle Models:
#     lora_weights_path = "path/to/your/downloaded/lora/weights"  #  e.g.,  "./model/gemma2-ko-dialogue-lora"
#  2. If you have the *.lora.h5 file from fine-tuning:
lora_weights_path = f"/kaggle/working/{lora_name}_{lora_rank}_epoch2.lora.h5"  # Or the correct epoch number.  Adjust the path if needed.


def text_gen(prompt, model):
    """Generates text based on the given prompt using the loaded model."""
    tick_start = time.time()
    input_text = f"<start_of_turn>user\n{prompt}<end_of_turn>\n<start_of_turn>model\n"
    output = model.generate(input_text, max_length=token_limit)
    print("\nGemma Output:")
    print(output)
    print(f"TOTAL TIME ELAPSED: {time.time() - tick_start:.2f}s")

# Load Base Model
gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(model_id)
gemma_lm.summary()

# Load LoRA Weights
gemma_lm.backbone.enable_lora(rank=lora_rank)

try:
    gemma_lm.backbone.load_lora_weights(lora_weights_path)
    print(f"LoRA weights loaded from {lora_weights_path}")
except Exception as e:
    print(f"Error loading LoRA weights: {e}")
    print("Make sure 'lora_weights_path' is correctly set to either:")
    print("  1. The path to your downloaded Kaggle Model (if you downloaded the LoRA weights).")
    print("  2. The path to the *.lora.h5 file generated by the fine-tuning notebook.")
    exit()  # Exit if weights cannot be loaded


# --- Inference Example ---
test_prompt = "요즘 기분이 너무 안 좋아."
text_gen(test_prompt, gemma_lm)

# --- Interactive Chat (Optional) ---
print("\n--- Interactive Chat (Type 'quit' to exit) ---")
while True:
    user_input = input("You: ")
    if user_input.lower() == "quit":
        break
    text_gen(user_input, gemma_lm)
